{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZg/EFSkDkQ+SgFkRe1ep1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bibowskii/LLM-T5-small-summarizer/blob/main/t5_tester.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score nltk"
      ],
      "metadata": {
        "id": "X4fkGl9QPq4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "SHAaKOggSLLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdEienLSPBSA"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import time\n",
        "from transformers import AdamW\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import re\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import nltk\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "nltk.download('punkt')  # Download necessary data for BLEU calculation\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "myModel = T5ForConditionalGeneration.from_pretrained(\n",
        "    '/content/drive/MyDrive/val/model',\n",
        "    local_files_only=True\n",
        ")\n",
        "print(myModel)\n",
        "tokenizer = T5Tokenizer.from_pretrained('google-t5/t5-small')"
      ],
      "metadata": {
        "id": "yGmanl1ZQl0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "testingSet = load_dataset(\"olanasir/cnn-daily-mail\", split=\"test\")\n",
        "validationSet = load_dataset(\"olanasir/cnn-daily-mail\", split=\"validation\")\n",
        "\n",
        "#checking the data\n",
        "\n",
        "print(testingSet[0])\n",
        "print(validationSet[0])"
      ],
      "metadata": {
        "id": "A20h3Cz1QtNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "myModel.to(device)\n",
        "print(device)"
      ],
      "metadata": {
        "id": "3q9MjrpxQzUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def tokenize_dataset(dataset):\n",
        "\n",
        "\n",
        "    inputs = tokenizer(dataset['article'], return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
        "    targets = tokenizer(dataset['highlights'], return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "    inputs['labels'] = targets['input_ids']\n",
        "    inputs['decoder_attention_mask'] = targets['attention_mask']\n",
        "    return inputs\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gpG70a6oQ52u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_testingSet = testingSet.map(tokenize_dataset, batched=True)\n",
        "tokenized_validationSet = validationSet.map(tokenize_dataset, batched=True)"
      ],
      "metadata": {
        "id": "LMoPqrwcQ629"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_testingSet.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels', 'decoder_attention_mask'])\n",
        "tokenized_validationSet.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels', 'decoder_attention_mask'])"
      ],
      "metadata": {
        "id": "Qpa45GwQQ_po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testingLoader = DataLoader(tokenized_testingSet, batch_size=8, shuffle= False, num_workers=2)\n",
        "validationLoader = DataLoader(tokenized_validationSet, batch_size=8, shuffle= False, num_workers=2)"
      ],
      "metadata": {
        "id": "oB3DL4i-RBd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def calculate_rouge(reference, hypothesis):\n",
        "  scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "  scores = scorer.score(reference, hypothesis)\n",
        "  return scores\n",
        "\n",
        "def calculate_bleu(reference, hypothesis):\n",
        "  reference_tokens = nltk.word_tokenize(reference)\n",
        "  hypothesis_tokens = nltk.word_tokenize(hypothesis)\n",
        "  bleu_score = sentence_bleu([reference_tokens], hypothesis_tokens)\n",
        "  return bleu_score"
      ],
      "metadata": {
        "id": "D3Qpsj3iPwwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_model(model, validationLoader, device):\n",
        "  model.eval()\n",
        "  total_loss = 0\n",
        "  with torch.no_grad():\n",
        "    for batch in validationLoader:\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      labels = batch['labels'].to(device)\n",
        "      outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "      loss = outputs.loss\n",
        "      total_loss += loss.item()\n",
        "  return total_loss / len(validationLoader)"
      ],
      "metadata": {
        "id": "38R6AtPSPpaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, testingLoader, tokenizer, device):\n",
        "    model.eval()  # Ensure model is in evaluation mode\n",
        "    predictions = []\n",
        "    all_references = []\n",
        "    all_hypotheses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in testingLoader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)  # Get the reference summaries\n",
        "\n",
        "            # Generate predictions from the model\n",
        "            outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            predictions.append(prediction)\n",
        "\n",
        "            # Decode reference summary\n",
        "            reference = tokenizer.decode(labels[0], skip_special_tokens=True)\n",
        "            all_references.append(reference)\n",
        "            all_hypotheses.append(prediction)\n",
        "\n",
        "    # Calculate ROUGE and BLEU scores for all predictions\n",
        "    rouge_scores = []\n",
        "    bleu_scores = []\n",
        "    for ref, hyp in zip(all_references, all_hypotheses):\n",
        "        rouge_scores.append(calculate_rouge(ref, hyp))\n",
        "        bleu_scores.append(calculate_bleu(ref, hyp))\n",
        "\n",
        "    # Calculate average scores\n",
        "    avg_rouge1 = sum([score['rouge1'].fmeasure for score in rouge_scores]) / len(rouge_scores)\n",
        "    avg_rouge2 = sum([score['rouge2'].fmeasure for score in rouge_scores]) / len(rouge_scores)\n",
        "    avg_rougeL = sum([score['rougeL'].fmeasure for score in rouge_scores]) / len(rouge_scores)\n",
        "    avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "\n",
        "    print(f\"Average ROUGE-1: {avg_rouge1:.4f}\")\n",
        "    print(f\"Average ROUGE-2: {avg_rouge2:.4f}\")\n",
        "    print(f\"Average ROUGE-L: {avg_rougeL:.4f}\")\n",
        "    print(f\"Average BLEU: {avg_bleu:.4f}\")\n",
        "\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "zEJLtaopPx6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "U8a19Pzzdhsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  validate_model(myModel, validationLoader, device)\n",
        "  test_model(myModel, testingLoader,tokenizer ,device)\n",
        "\n"
      ],
      "metadata": {
        "id": "b9hfRDifP2uB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def summarize_text(text):\n",
        "    # Prepare the input text for the model\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "    # Move tensors to the correct device (GPU or CPU)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    myModel.to(device)\n",
        "\n",
        "    # Generate summary\n",
        "    summary_ids = myModel.generate(inputs['input_ids'].to(device), attention_mask=inputs['attention_mask'].to(device), max_length=130, min_length=30, do_sample=False)\n",
        "\n",
        "    # Decode the summary\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Test the summarizer with manual text input\n",
        "while True:\n",
        "    text = input(\"Enter the text to summarize (or type 'exit' to quit): \")\n",
        "    if text.lower() == 'exit':\n",
        "        break\n",
        "    print(\"\\nSummary:\", summarize_text(text))"
      ],
      "metadata": {
        "id": "sYnwzPY7P9j2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}